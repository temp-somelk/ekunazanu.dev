<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="Anchit Roy" name=author><meta content="Bloom Filters, Count-min Sketches and HyperLogLog." name=description><meta content="Probabilistic Data Structures" property=og:title><meta content=article property=og:type><meta content="Bloom Filters, Count-min Sketches and HyperLogLog." property=og:description><meta content=https://ekunazanu.dev/lab/probabilistic-data-structures/ property=og:url><meta content=https://ekunazanu.dev/media/thumbnails/lab.die.svg.png property=og:image><meta content="Die with five dots visible on its one square face." property=og:image:alt><meta content=image/png property=og:image:type><meta content=1200 property=og:image:width><meta content=900 property=og:image:height><meta content=en_US property=og:locale><meta content=ekunazanu property=og:site_name><title>Probabilistic Data Structures</title><link href=https://ekunazanu.dev/lab/probabilistic-data-structures/ rel=cannonical><link href=https://ekunazanu.dev/atom.xml rel=alternate type=application/atom+xml><link href=https://ekunazanu.dev/misc/main.css rel=stylesheet><link href=https://ekunazanu.dev/misc/lab.favicon.png rel=icon><meta content=Zola name=generator><body><nav><ul><li><h2><a href=https://ekunazanu.dev>ekunazanu.dev</a></h2><li><a href=https://ekunazanu.dev/log>Log</a><li><a href=https://ekunazanu.dev/lab>Lab</a> ⟶</ul></nav><div class=print>https://ekunazanu.dev</div><main><article><h1>Probabilistic Data Structures</h1><p>Probabilistic data structures are what their names suggest — these are <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Data_structure target=_blank>data structures</a> that give probabilisitic answers to queries. What is lost in <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Accuracy_and_precision#/media/File:Accuracy_and_precision.svg target=_blank>precision</a> is more than made up for in extremely efficient use of memory and/or computational resources.<h2 id=Hashing>Hashing</h2><p>A hash function is simply a mathematical function that maps an input to an output. In practice however, the output of hash functions have certain properties:<ul><li><strong>Deterministic</strong> — Hash functions produce the same output for the same input.<li><strong>Fixed Length</strong> — For any input, most hash functions produce a fixed length output.<li><strong>Pseudorandom</strong> — Most hash functions produce a hash that is <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Pseudorandomness target=_blank>pseudorandom</a>; hash outputs appear and behave random, but are ultimately deterministic.<li><strong>One-Way</strong> — For most hash functions, it is difficult to determine the input just from the output. This is useful for <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Cryptographic_hash_function target=_blank>cryptographic applications</a>; this is not necessarily useful for probabilistic data structrues.<li><strong>Collision Resistant</strong> — Despite having fixed length ouputs, hash functions are designed to minimize cases where two inputs output the same hash. Collisions are not impossible however, because of the <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Pigeonhole_principle target=_blank>limited output space</a>.</ul><p>Some of these properties are immensely useful and form the basis of most probabilistic data structures.</p><input placeholder="Enter some input"><button>Hash</button><br> hash("hello"): <code>0x2E</code><h2 id=Bloom_Filters>Bloom Filters</h2><p>Bloom filters are one of the most popular probabilistic data structures, that are used to check for membership in sets or multisets. In simpler words, bloom filters are used to check if an element is present in a set or not. Bloom filters cannot identify if an element exists in a set with absolute certainty, but it can report with certainty if it does <strong>not</strong> exist in a set. And it does it for big datasets using very little space.<h3 id=Approaches>Approaches</h3><p>Rather than outright disclosing how bloom filters work, it is much more effective to discuss possible solutions first. It will help build a natural intuition as to why bloom filters are a better approach — at least when accuracy is not a big priority.<p>Consider these elements:</p><span style="font:normal .9rem var(--monospace);border:1px solid var(--fg);margin:1rem 0;padding:1rem;line-height:1.25rem;display:block;overflow:auto"> world firm bat if glance analysis reasonable resident verdict world snub greet snub half speed exception speed helmet theorist please operational hello nursery background appreciate congress verdict dictionary current nursery snub piece dilute elapse congress verdict confusion fan breast sting disagreement helmet tape </span><p>We need to know if <code>hello</code> is in the multiset. One possible solution is to store the entire multiset, and then performing a <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Linear_search target=_blank>linear search</a> — going through each element one-by-one and checking if it exists.</p><span style="font:normal .9rem var(--monospace);border:1px solid var(--fg);margin:1rem 0;padding:1rem;line-height:1.25rem;display:block;overflow:auto"> world firm bat if glance analysis reasonable resident verdict world snub greet snub half speed exception speed helmet theorist please operational hello nursery background appreciate congress verdict dictionary current nursery snub piece dilute elapse congress verdict confusion fan breast sting disagreement helmet tape </span><p>Or if you’ve studied computer science, the multiset can be sorted in <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Lexicographic_order target=_blank>lexicographic order</a> and then a <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Binary_search_algorithm target=_blank>binary search</a> can be performed.</p><span style="font:normal .9rem var(--monospace);border:1px solid var(--fg);margin:1rem 0;padding:1rem;line-height:1.25rem;display:block;overflow:auto"> world firm bat if glance analysis reasonable resident verdict world snub greet snub half speed exception speed helmet theorist please operational hello nursery background appreciate congress verdict dictionary current nursery snub piece dilute elapse congress verdict confusion fan breast sting disagreement helmet tape </span><p>That makes querying faster, but a lot of space is still needed to store all the elements. It can be optimized on that front. One way to reduce the space used could be by storing only the unique values and purging all the duplicates:</p><span style="font:normal .9rem var(--monospace);border:1px solid var(--fg);margin:1rem 0;padding:1rem;line-height:1.25rem;display:block;overflow:auto"> world firm bat if glance analysis reasonable resident verdict world snub greet snub half speed exception speed helmet theorist please operational hello nursery background appreciate congress verdict dictionary current nursery snub piece dilute elapse congress verdict confusion fan breast sting disagreement helmet tape </span><p>But the space required will still scale linearly with the number of elements and the size of each element. What if, instead of storing all the individual elements, only the hashes of the elements are stored instead? In this case, we’ll need to search for <code>0x2E</code> — the hash for <code>hello</code>.</p><span style="font:normal .9rem var(--monospace);border:1px solid var(--fg);margin:1rem 0;padding:1rem;line-height:1.25rem;display:block;overflow:auto"> world firm bat if glance analysis reasonable resident verdict world snub greet snub half speed exception speed helmet theorist please operational hello nursery background appreciate congress verdict dictionary current nursery snub piece dilute elapse congress verdict confusion fan breast sting disagreement helmet tape </span><p>Using hashes is arguably better than storing individual long variable-length elements. But it can be optimized even further. Since most hashing functions always produce a fixed length output for any input, it can be exploited to decrease the amount of space needed to store the hashes even more.<p>Consider the above example. The hash function produces an output between <code>0x00</code> and <code>0xFF</code>. In decimal, it is between 0 and 255. Instead of storing all the hashes individually, a <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Bit_array target=_blank>bit-array</a> of 256-bits can be created (with all bits initially set to zero). A bit can be set to one, for its corresponding hash.<p>For example if the output hash is <code>0x5E</code> (94 in decimal), the 94th bit is flipped to one.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>It should be obvious that using a bit-array is analogous to storing the hash itself. But while in the previous approach multiple bits were needed to store the individual hashes, the new bit-array approach requires only one bit per hash.<p>And this is what a bloom filter is. It is fundamentally a bit-array, where a bit corresponds to an individual hash output — and that hash output correspondings to an element (or more elements) in a set. The only difference is that multiple hash functions are used in bloom filters to minimize collisions.<h3 id=Querying>Querying</h3><p>It is apparent that checking for membership of an element simply involves verifying if the hash exists — checking if the corresponding bit in the bit-array is set to one. So to query an element: It is first hashed, and then the bit in the bit-array corresponding to the hash is assessed to see if it is set (to one).</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><h3 id=Hash_Collisions>Hash Collisions</h3><p>While hash functions are designed to be collision resistant, they are not collision proof. Sometimes two elements may have the same hash. In such cases, an element that was added may have the same hash as a different element being queried, leading to a false positive.<p>For example, <code>hello</code> having the hash <code>0x2E</code> is added in the bloom filter. Another word <code>bye</code> might have the same hash <code>0x2E</code>. If the bloom filter is queried for <code>bye</code>, it would wrongly report that <code>bye</code> exists in the set — since the bit for <code>0x2E</code> in the bit-array is already set to one by a different word <code>hello</code>.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>To reduce the probability of collisions, multiple hash functions can be used for each element. In that case, querying an element will return true only if <strong>all</strong> bits corresponding to the hashes are one.<p>The probability of two elements having the same hash outputs is <code>1/m</code> where <code>m</code> is the size of the bloom filter. However, if two hash functions are used for every element, the probability of two elements having the same hash outputs is roughly <code>1/m * 1/m</code>. In general, using more hashing functions exponentially decreases the probability that <strong>all</strong> the hash outputs of two elements will collide.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><details open><summary>Universal hashing</summary> The assumption is that the output of the hash functions are randomly distributed and are not correlated to each other. In other words, the hash functions should be selected from a <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Universal_hashing target=_blank>universal family</a>, and ideally be <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Pairwise_independence target=_blank>pairwise indepenedent</a>.</details><p>However using too many hashing functions can also increase the false positive rate. It might feel counterintuitive, especially right after showing it <em>decreases</em> false positives, but consider this: The bloom filter is finite. The more the number of hash functions, the higher the probability that it fills up quickly and most bits are set to one. When querying from a bloom filter where most bits are set to one, the higher the probability that any query results in a positive — even if the element is not in the set.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>In the extreme case when all the bits in a bloom filter are set to one, the bloom filter would <strong>always</strong> report positive for membership — regardless of whether the element actually exists in the set.<p>Other than using a large number of hashing functions, a bloom filter can also be quickly saturated if the number elements to be hashed (added) is huge. The only solution to decreasing the number of false positives then, is by increasing the size of the bloom filter itself. Increasing the size of the bloom filter bit-array equates to a larger output space for the hash functions, reducing the probability for collisions.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>Thus the false positive rate is also dependent on the size of the bloom filter and the number of (unique) elements to be added.<h3 id=Space_Efficiency>Space Efficiency</h3><p>Thus, the false positive rate depends on the number of hash functions used per element, the number of elements hashed/added, and the size of the bloom filter. Conversely, the size of the bloom filter and the ideal number of hash functions depends on the tolerance for false positives and the number elements to be added.<p>More precisely, the <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Bloom_filter#Optimal_number_of_hash_functions target=_blank>optimal size for a bloom filter</a> is <code>-2.08·ln(p)·n</code> bits, where <code>p</code> is the tolerance for the false positive rate and <code>n</code> is the number of unique elements one expects to observe. The <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Bloom_filter#Optimal_number_of_hash_functions target=_blank>optimal number of hash functions</a> is <code>ln(2)·(m/n)</code>, where <code>m</code> is the size of the bloom filter and <code>n</code> is the number of expected elements. Thomas Hurst visualizes how the factors affect each other in his <a rel="noopener nofollow noreferrer" href=https://hur.st/bloomfilter/ target=_blank>bloom filter calculator</a>.<h3 id=Deletion>Deletion</h3><p>Deletions can be achieved by unsetting the bits corresponding to the hash of the element to be deleted, to zero. But if one of the bits was previously set by some different element, then checking membership for that element would lead to a false negative.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>The general consensus is to forbid deletions in bloom filters to remove the possibility of getting false negatives.<h2 id=Count-Min_Sketch>Count-Min Sketch</h2><p>A count-min sketch is a probabilisitc data structure that calculates the maximum occurences of elements in a set. It is what a bloom filter would have been if it was a frequency table instead.<h3 id=Counting_Bloom_Filter>Counting Bloom Filter</h3><p>Count-min sketches are very similar to bloom filters, so it’s helpful to start off with a bloom filter.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>Instead of using one bit per hash, what if a few more bits per hash were used instead?</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>A way to utilize the extra bits is use it as ‘counters’ to store information about frequency. The bit-counter can be incremented by one whenever a hash (and therefore, an element) is added. This expands the ability of the array from being able to only store information about the <strong>existence</strong> of an element to store information about its <strong>frequency</strong> as well.<pre>
.---+---+---+---+---+---+---+---.
| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |   Existence
'---+---+---+---+---+---+---+---'
.-----+-----+-----+-----+-----+-----+-----+-----.
| 000 | 000 | 000 | 000 | 000 | 000 | 000 | 000 |   Frequency
'-----+-----+-----+-----+-----+-----+-----+-----'

<input placeholder="Add element"> <button>Hash and add to set</button>
<input placeholder="Search element"> <button>Hash and query frequency</button>
</pre><p>The array on top is a bloom filter, used for querying membership. The one below that is a counting bloom filter which helps us query frequency information as well.<p>In a counting bloom filter, the counter for the hash of an element is incremented when an element is added. However the counters can <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Integer_overflow target=_blank>overflow</a> and reset back to zero, leading to wrong estimates. This is not a big issue however, since even thirty-two bits per counter is enough for storing frequency estimates up to roughly four billion.<h3 id=Hash_Collisions-1>Hash Collisions</h3><p>The other (and more susceptible) way error seeps through in the estimates is via hash collisions. Sometimes two elements will map to the same hash, increasing the count of the other element. This means elements can be over-counted. However there is no way for elements to be under-counted (barring overflows).<pre>
.-----+-----+-----+-----+-----+-----+-----+-----.
| 000 | 000 | 000 | 000 | 000 | 000 | 000 | 000 |
'-----+-----+-----+-----+-----+-----+-----+-----'

Hash(A) = Hash(C)
<button>Add A to set</button> <button>Query frequency of A</button>
<button>Add B to set</button> <button>Query frequency of B</button>
<button>Add C to set</button> <button>Query frequency of C</button>
Actual frequency of A: 0
Actual frequency of B: 0
Actual frequency of C: 0
</pre><p>To reduce errors, approaches used in bloom filters can be applied: By increasing the size of the array, and using multiple hash functions. The solutions might look plausible on the surface because of the counting bloom filter’s similiarities to a bloom filter, but that is not the case — or at least, it is not that simple.<p>The first solution is pretty easy to verify. Increasing the size of the array increases the range of the hash functions, reducing collisions — regardless of the number of hashing functions used. The reasoning is similar to bloom filters, discussed above.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>Similar to bloom filters, when querying using multiple hash functions per element, the intersection of all the counters and using the minimum value of all the counters is used. Since collisions only cause elements to be over-counted, the minimum estimate is the closest to the true frequency.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>It might seem that it using multiple hash functions would reduce errors similar to bloom filters. However when it comes to increasing the number of hash functions, it does not decrease errors — in fact it actually leads to slightly higher errors.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>It’s important to mention that in both bloom filters and counting bloom filters, increasing the number of hash functions per element increases the probability of a collision. But in the case of bloom filters, any hash collision did not change the value of a bit in the bloom filter itself — it would be set to one regardless. In fact hash collisions <a rel="noopener nofollow noreferrer" href=https://stackoverflow.com/a/72509014 target=_blank>decrease overall error</a> by preventing the bloom filter from getting saturated too quickly.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>Compare that to a counting bloom filter, where every collision increases the value of the counter by one. In the case of a single hash function being used, a counter would get incremented by one. However if multiple hash functions are used, the probability when an element with multiple hash functions would map to the same counter is non-zero. Adding an element to the sketch would then increment the counter by more than one. And worse, the probability of over-counting an element only increases as the number of hash functions per element increases.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>The solution to reducing errors then would be to increase the size of the array and to stick to using a single hash function. And while that is acceptable, there are better ways to go about it.<h3 id=Separate_Hash_Ranges>Separate Hash Ranges</h3><p>There is an upside in using multiple hash functions — while querying the frequency of an element, it gets expenontially less probable that the <strong>all</strong> hashes of <strong>one</strong> element will collide. The downside to using more hash functions is that it increases <strong>overall</strong> collisions, which leads to over-counting and less precise estimates.<p>A simple way to resolve the downside is by assigning the ouput of each hash function to separate output spaces. This prevents from collisions between hash functions. That is, while collisions from different elements having the same hashes are possible, collisions between different hash functions are now impossible.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>The resulting data structure is a count-min sketch. It can also be thought of as a two dimensional array with <code>w</code> columns and <code>d</code> rows — where <code>d</code> is the total number of hash functions and <code>w</code> is the range for the hash outputs. Each hash function outputs to their respective row, and the hash functions are again assumed to be pairwise independent.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>The count-min sketch is capable of answering more than simple frequency queries. It can also respond to <a href="https://cs.stanford.edu/~rishig/courses/ref/l12b.pdf#page=5" rel="noopener nofollow noreferrer" target=_blank>range and inner-product queries</a>.<h3 id=Space_efficiency>Space efficiency</h3><p>Similar to bloom filters, the space required by a count-min depends on <a rel="noopener nofollow noreferrer" href=http://dimacs.rutgers.edu/~graham/pubs/papers/cmsoft.pdf target=_blank>the tolerance for error and the number of elements to be hashed</a>. The optimal width of the array <code>w</code> is <code>2n/ε</code> and depth <code>d</code> is <code>log(δ)/log(1/2)</code> — where <code>n</code> is the number of elements, <code>ε</code> defines the bounds for frequency estimates, and <code>δ</code> is the probability the estimates exceeds those bounds.<h2 id=HyperLogLog>HyperLogLog</h2><p>HyperLogLog is another probabilistic data structure, used to approximate the cardinality, or total number of distinct elements in a multiset. It relies on the pseudorandomness of the hashes as its basis to calculate the cardinality of a multiset.<h3 id=Probability>Probability</h3><p>Consider this thought expirement: Take a few coins where each coin has an equal probability of landing on heads or tails and toss them all at once. The probability that the first coin is heads is half, or one in <code>2</code>. The probability that both the first and second coin are heads is one in <code>4</code>. The probability that all three first coins are heads is one in <code>8</code>. In general, for all the first <code>n</code> coins to be heads, the probability is one in <code>2^n</code>.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>Alternatively, the coins need to be tossed roughly <code>2^n</code> times to observe <code>n</code> consecutive heads at the beginning of the coin array.<p>Conversely, if shown <code>n</code> consecutive heads, it can be estimated that the coins were tossed roughly <code>2^n</code> times.<p>The maximum number of consecutive heads returns a rough estimate for the number of times the coins were tossed.<p>This is the fundamental principle behind HyperLogLog. Coins are replaced with bits — a hash function is chosen such that output is pseudorandom. Each bit of the hash acts as an unbiased coin — having an equal probability of being <code>0</code> or <code>1</code>. The total number of elements can be estimated using the length of the longest run of consecutive zeroes of their hashes.</p><input placeholder="Enter some input"><button>Hash</button><button>Hash random element</button> hash(1) = 00000101 <span style=color:#129;font-size:1rem;font-weight:800>5</span> consecutive 0s suggests roughly <span style=color:#912;font-size:1rem;font-weight:800>32</span> elements hashed. Actual number of unique elements: 35 <button>Reset</button><p>Since hashes are deterministic, duplicate elements will yield the same hashes and would not affect the cardinality calculation. So HyperLogLog estimates the number of <strong>distinct</strong> elements in a multiset.<h3 id=Weakness>Weakness</h3><p>There is a glaring flaw in this approach however. The gaps between the approximations double every time an extra consecutive zero is observed — it can only provide estimates that are powers of two, and nothing in between. Second, a hash output may have a lot of consecutive zeros at the beginning simply owing to chance, causing the estimate to be wildly off and beyond the true cardinality.<p>To mitigate these problems, multiple counters can be used that store the longest run of zeroes for different subsets of the multiset — instead of storing one counter that stores the longest run of zeroes for the entire multiset. The values can then be averaged to find a more <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Accuracy_and_precision#/media/File:Accuracy_and_precision.svg target=_blank>accurate</a> cardinality estimate.<h3 id=Buckets>Buckets</h3><p>A multiset is segregated into different subsets using ‘<a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Hash_table#/media/File:Hash_table_3_1_1_0_1_0_0_SP.svg target=_blank>buckets</a>’. In HyperLogLog, a bucket is just a container for storing the length of the longest run of zeroes of hashes for some particular subset of elements. The first <code>b</code> bits of the hash of an element are reserved for selecting the bucket to update the values, and the remaining hash is used for finding the longest run of consecutive zeroes for that bucket.<pre>
<input placeholder="Enter some input"> <button>Add</button> <button>Add random element</button>
hash(1) = 11000001
+-------------------------------+
| <span style=color:#192;font-size:1rem;font-weight:800>1</span> | <span style=color:#192;font-size:1rem;font-weight:800>0</span> | <span style=color:#129;font-size:16px;font-weight:800>0</span> | <span style=color:#129;font-size:1rem;font-weight:800>0</span> | 1 | 1 | 0 | 1 |
+-------------------------------+

<span style=color:#129;font-size:1rem;font-weight:800>2</span> consecutive 0s observed.
Bucket <span style=color:#192;font-weight:800>10</span> updated|not updated.

Buckets:
+---------------+
| 3 | 2 | 3 | 4 |           (Blue color number if updated)
+---------------+
<span style=color:#192> 00  01  10  11</span>
</pre><p>Each bucket stores the longest run of zeroes for a subset of elements — which is used to compute the cardinality estimate for that bucket. Cardinality for the entire multiset can be calculated by finding the average of the individual estimates using the <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Harmonic_mean target=_blank>harmonic mean</a>.<pre>
Buckets:
+---------------+
| 3 | 2 | 3 | 4 |
+---------------+

<button>Calculate cardinality</button>

Cardinality estimate of bucket <span style=color:#192>00</span>: <span style=color:#912;font-weight:800>8</span>
Cardinality estimate of bucket <span style=color:#192>01</span>: <span style=color:#912;font-weight:800>4</span>
Cardinality estimate of bucket <span style=color:#192>10</span>: <span style=color:#912;font-weight:800>8</span>
Cardinality estimate of bucket <span style=color:#192>11</span>: <span style=color:#912;font-weight:800>16</span>

Harmonic mean of all the estimates: 7
Actual number of unique elements: 6
<button>Reset</button>
</pre><details open><summary>Averaging using harmonic mean</summary> The harmonic mean is used for averaging because it reduces the influence of large outliers from affecting the estimate.</details><h3 id=Scaling_and_Discarding>Scaling and Discarding</h3><p>There is a predictable <a rel="noopener nofollow noreferrer" href=https://www.moderndescartes.com/essays/hyperloglog/#loglog target=_blank>bias</a> towards the larger estimates, and scaling the final estimate by a correction factor that ranges from <code>0.673</code> to <code>0.709</code> results in estimates that are much closer to the true cardinality. <a rel="noopener nofollow noreferrer" href=https://dl.acm.org/doi/abs/10.1145/2452376.2452456 target=_blank>Research</a> also shows that discarding the largest 30% of the values in the buckets before averaging can improve the accuracy of the estimates.<p>These corrective measures can bring down the error rate to <code>1.04/√m</code>, where <code>m</code> is the number of buckets.<p>This is HyperLogLog. Adding elements involves hashing them and storing their longest run of leading zeroes in buckets. To estimating the cardinality, the harmonic mean of the smallest 70% of values is taken and then scaled by a correction factor.<pre>
<input placeholder="Enter some input"> <button>Add</button> <button>Add random element</button>
hash(1) = 11000001
+---------------------------------------------------------------+
| <span style=color:#192;font-size:1rem;font-weight:800>1</span> | <span style=color:#192;font-size:1rem;font-weight:800>0</span> | <span style=color:#192;font-size:1rem;font-weight:800>1</span> | <span style=color:#129;font-size:16px;font-weight:800>0</span> | <span style=color:#129;font-size:1rem;font-weight:800>0</span> | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 1 | 0 |
+---------------------------------------------------------------+

<span style=color:#129;font-size:1rem;font-weight:800>2</span> consecutive 0s observed.
Bucket <span style=color:#192;font-weight:800>101</span> updated|not updated.

Buckets:
+-------------------------------+
| 4 | 2 | 4 | 4 | 3 | 2 | 1 | 5 |               (Blue color number if updated)
+-------------------------------+

<button>Calculate cardinality</button>

Cardinality estimate of bucket <span style=color:#192>000</span>: <span style=color:#912;font-weight:800>16</span>
Cardinality estimate of bucket <span style=color:#192>001</span>: <span style=color:#912;font-weight:800>4</span>
Cardinality estimate of bucket <span style=color:#192>010</span>: <span style=color:#912;font-weight:800>16</span>
Cardinality estimate of bucket <span style=color:#192>011</span>: <span style=color:#912;font-weight:800>16</span>
Cardinality estimate of bucket <span style=color:#192>100</span>: <span style=color:#912;font-weight:800>8</span>
Cardinality estimate of bucket <span style=color:#192>101</span>: <span style=color:#912;font-weight:800>4</span>
Cardinality estimate of bucket <span style=color:#192>110</span>: <span style=color:#912;font-weight:800>2</span>
Cardinality estimate of bucket <span style=color:#192>111</span>: <span style=color:#912;font-weight:800>32</span>

Harmonic mean of 70% of the smallest estimates: 6
Estimate after being scaled by the correction factor: 5

Final cardinality estimate: 5

Actual number of unique elements: 6

<button>Reset</button>
</pre><p>It is less acccurate when the number of elements are low, and becomes more accurate as the true cardinality grows.<h3 id=Space_Efficiency-1>Space Efficiency</h3><p>The only thing needed (to store) for calculating the cardinality are the buckets. So the total amount of space required is the number of buckets times the size of each bucket. The number of buckets depends on the degree of accuracy required — the error rate is inversely proportional to the sqaure root of quantity of buckets. More buckets lead to more accurate estimates and vice versa.<p>The largest number that can be stored in each bucket before it overflows depends on its size. A five-bit bucket can store numbers from zero to thirty-one. A hash with thirty-one leading zeroes in a hash suggests roughly two billion unique elements were hashed. So to store cardinalities of up to two billion unique elements, the size of each bucket needs to be only five bits. To store cardinality of <code>n</code> unique elements, the size of each bucket needs to be <code>log(log(n))</code> bits. That is also where its name comes from.</p><svg viewbox="0 0 200 75" fill=none height=450px id=bitarraySVG width=1200px><desc>Bit array.</desc><rect fill=#aaa height=150 id=bitarrayBG width=200 x=0 y=0 /></svg><p>For example, HyperLogLog is able to estimate cardinalities of more than a billion unique elements with an an error of 2% using 2500 five-bit buckets — only 1.5 kB of memory. Or alternatively, just 0.25 kB for an error of 5% using 400 buckets.<h2 id=Other_Data_Structures>Other Data Structures</h2><p>There are lots of other probabilistic data structures that trade accuracy for efficiency. In addition to bloom filters, both <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Cuckoo_filter target=_blank>cuckoo filters</a> or <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Quotient_filter target=_blank>quotient filers</a> are useful probabilistic data structures used for membership queries. Or <a rel="noopener nofollow noreferrer" href=https://doi.org/10.1145/78922.78925 target=_blank>linear counting</a>, which is used for estimating distinct elements — but the underlying principle is similar to bloom filters. Rank can be approximated using <a rel="noopener nofollow noreferrer" href=https://doi.org/10.48550/arXiv.1902.04023 target=_blank>t-digests</a>, or <a rel="noopener nofollow noreferrer" href=https://doi.org/10.48550/arXiv.1603.05346 target=_blank>KLL sketches</a>, and similarities can be estimated using <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Locality-sensitive_hashing target=_blank>LSH</a>, <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/MinHash target=_blank>MinHash</a>, and <a rel="noopener nofollow noreferrer" href=https://doi.org/10.1145/509907.509965 target=_blank>SimHash</a>. There’s a lot of other resources online to know more about the different kinds of probabilisitc data structures that exist.<hr><h2 id=References>References</h2><ul><li>Florian Hartmann: <a rel="noopener nofollow noreferrer" href=https://florian.github.io/bloom-filters/ target=_blank>Bloom Filters</a><li>Eric Crahen: <a rel="noopener nofollow noreferrer" href=https://crahen.github.io/algorithm/stream/count-min-sketch-point-query.html target=_blank>Count-Min Sketching, Configuration & Point-Queries</a><li>Engineering at Meta: <a rel="noopener nofollow noreferrer" href=https://engineering.fb.com/2018/12/13/data-infrastructure/hyperloglog/ target=_blank>HyperLogLog in Presto: A significantly faster way to handle cardinality estimation</a><li>Cheng-Wei Hu: <a rel="noopener nofollow noreferrer" href=https://chengweihu.com/hyperloglog/ target=_blank>HyperLogLog: A Simple but Powerful Algorithm for Data Scientists</a></ul></article></main><footer><a href=https://ekunazanu.dev/more#terms-of-use>© 2024</a> <a href=https://ekunazanu.dev>Anchit Roy</a> · <a rel="noopener nofollow noreferrer" href=https://creativecommons.org/licenses/by/4.0/ target=_blank>CC BY 4.0</a></footer>